{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"WANDB_API_KEY\"] = \"7b14a62f11dc360ce036cf59b53df0c12cd87f5a\"\n",
    "import torch\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from lora import LoRANetwork\n",
    "from diffusers.utils import export_to_gif\n",
    "from shap_e.diffusion.sample import sample_latents\n",
    "from shap_e.models.download import load_model, load_config\n",
    "from shap_e.diffusion.gaussian_diffusion import diffusion_from_config\n",
    "from shap_e.util.notebooks import create_pan_cameras, decode_latent_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5\n",
    "rank = 4\n",
    "size = 160\n",
    "prompt = \"\"\n",
    "alpha = 1.0\n",
    "sigma_max = 160\n",
    "scales = [-1, 1]\n",
    "render_mode = \"nerf\"\n",
    "cond_drop_prob = 0.5\n",
    "guidance_scale = 7.5\n",
    "name = \"armsslider_2024-03-23 13:46:14.541733\"\n",
    "lora_weight = f\"/home/noamatia/repos/spic-e/outputs/{name}/model_final.pt\"\n",
    "\n",
    "output_dir = os.path.join('outputs', name, 'test')\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "def flush(*args):\n",
    "    for arg in args:\n",
    "        del arg\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create LoRA for SplitVectorDiffusion: 245 modules.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fdb510df8254e2aa364a2995e1ca968",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74fb7c8c14ba43ccae7182f69e4b4bd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74e43b9f485e4dcd85bec4968a0af041",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd891a365cfd48fba4e6e67094e84c00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f1083f21b464d07a5f38137f78b8a3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "438a6966d6d04eb78715ae8082bea4f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40e03a8bcaf540c6a04a6b2ac703eb15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f59659f4040456eb3a3658b894d2ba4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "354998c41e9d4fb0aa5cbc06c2399649",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [04:58<19:52, 298.23s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3435fcd54614438ca8da4adae0c1d5c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbdda3839b6243c499cbd1fb6cebbd76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89812378698d4342b627b6d4aee7fca1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffea0f4c8f00442385560e91ab9024de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aadf75817f4b406a80097a88b801e725",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4ba94ca85d34e49aed056c6f8920856",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "093014b93d8c4410b11854531f2c6ae4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c70413c09ccf4ae6aaba65f2f4bdc395",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "595314a7b2534a06b91c48aeae63f864",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [09:52<14:48, 296.14s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b4716fd4c7a4af78e4dab547494ec51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [10:11<15:17, 305.95s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 36\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     19\u001b[0m         test_latents \u001b[38;5;241m=\u001b[39m sample_latents(\n\u001b[1;32m     20\u001b[0m             device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[1;32m     21\u001b[0m             batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     34\u001b[0m             x_T\u001b[38;5;241m=\u001b[39mx_T,\n\u001b[1;32m     35\u001b[0m         )\n\u001b[0;32m---> 36\u001b[0m images \u001b[38;5;241m=\u001b[39m \u001b[43mdecode_latent_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_latents\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcameras\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrendering_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrender_mode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m result_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscale\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.gif\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     38\u001b[0m export_to_gif(images, result_path)\n",
      "File \u001b[0;32m~/miniconda3/envs/spic-e/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/spic-e/shap_e/util/notebooks.py:54\u001b[0m, in \u001b[0;36mdecode_latent_images\u001b[0;34m(xm, latent, cameras, rendering_mode, background_color)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mno_grad()\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode_latent_images\u001b[39m(\n\u001b[1;32m     48\u001b[0m     xm: Union[Transmitter, VectorDecoder],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     52\u001b[0m     background_color: torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m255.0\u001b[39m, \u001b[38;5;241m255.0\u001b[39m, \u001b[38;5;241m255.0\u001b[39m]),\n\u001b[1;32m     53\u001b[0m ):\n\u001b[0;32m---> 54\u001b[0m     decoded \u001b[38;5;241m=\u001b[39m \u001b[43mxm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrenderer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender_views\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43mAttrDict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcameras\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcameras\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mxm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mxm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTransmitter\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mxm\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbottleneck_to_params\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlatent\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mAttrDict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrendering_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrendering_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrender_with_direction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m     bg_color \u001b[38;5;241m=\u001b[39m background_color\u001b[38;5;241m.\u001b[39mto(decoded\u001b[38;5;241m.\u001b[39mchannels\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     62\u001b[0m     arr \u001b[38;5;241m=\u001b[39m bg_color \u001b[38;5;241m*\u001b[39m decoded\u001b[38;5;241m.\u001b[39mtransmittance \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m decoded\u001b[38;5;241m.\u001b[39mtransmittance) \u001b[38;5;241m*\u001b[39m decoded\u001b[38;5;241m.\u001b[39mchannels\n",
      "File \u001b[0;32m~/repos/spic-e/shap_e/models/nerstf/renderer.py:215\u001b[0m, in \u001b[0;36mNeRSTFRenderer.render_views\u001b[0;34m(self, batch, params, options)\u001b[0m\n\u001b[1;32m    211\u001b[0m rendering_mode \u001b[38;5;241m=\u001b[39m options\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrendering_mode\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rendering_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnerf\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mrender_views_from_rays\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender_rays\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m rendering_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstf\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    225\u001b[0m     sdf_fn \u001b[38;5;241m=\u001b[39m tf_fn \u001b[38;5;241m=\u001b[39m nerstf_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/repos/spic-e/shap_e/models/renderer.py:217\u001b[0m, in \u001b[0;36mrender_views_from_rays\u001b[0;34m(render_rays, batch, params, options, device)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_batches):\n\u001b[1;32m    213\u001b[0m     rays_batch \u001b[38;5;241m=\u001b[39m AttrDict(\n\u001b[1;32m    214\u001b[0m         rays\u001b[38;5;241m=\u001b[39mrays[:, idx \u001b[38;5;241m*\u001b[39m ray_batch_size : (idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m ray_batch_size],\n\u001b[1;32m    215\u001b[0m         radii\u001b[38;5;241m=\u001b[39mradii[:, idx \u001b[38;5;241m*\u001b[39m ray_batch_size : (idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m ray_batch_size],\n\u001b[1;32m    216\u001b[0m     )\n\u001b[0;32m--> 217\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mrender_rays\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrays_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(camera, DifferentiableProjectiveCamera):\n\u001b[1;32m    220\u001b[0m         z_batch \u001b[38;5;241m=\u001b[39m z_directions[:, idx \u001b[38;5;241m*\u001b[39m ray_batch_size : (idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m ray_batch_size]\n",
      "File \u001b[0;32m~/repos/spic-e/shap_e/models/nerstf/renderer.py:141\u001b[0m, in \u001b[0;36mNeRSTFRenderer.render_rays\u001b[0;34m(self, batch, params, options)\u001b[0m\n\u001b[1;32m    132\u001b[0m options\u001b[38;5;241m.\u001b[39mnerf_level \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfine\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    133\u001b[0m parts \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    134\u001b[0m     RayVolumeIntegral(\n\u001b[1;32m    135\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    139\u001b[0m     ),\n\u001b[1;32m    140\u001b[0m ]\n\u001b[0;32m--> 141\u001b[0m fine_results, _, raw_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mrender_rays\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrays\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvoid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshared\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseparate_shared_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprev_raw_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoarse_raw_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrender_with_direction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender_with_direction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m raw \u001b[38;5;241m=\u001b[39m raw_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    151\u001b[0m aux_losses \u001b[38;5;241m=\u001b[39m fine_results\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39maux_losses\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[0;32m~/repos/spic-e/shap_e/models/nerf/ray.py:79\u001b[0m, in \u001b[0;36mrender_rays\u001b[0;34m(rays, parts, void_model, shared, prev_raw_outputs, render_with_direction, importance_sampling_options)\u001b[0m\n\u001b[1;32m     74\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m part_i, prev_raw_i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(parts, prev_raw_outputs):\n\u001b[1;32m     77\u001b[0m \n\u001b[1;32m     78\u001b[0m     \u001b[38;5;66;03m# Integrate over [t[i], t[i + 1]]\u001b[39;00m\n\u001b[0;32m---> 79\u001b[0m     results_i \u001b[38;5;241m=\u001b[39m \u001b[43mpart_i\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender_rays\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m        \u001b[49m\u001b[43morigin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdirec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mt0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprev_raw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprev_raw_i\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshared\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshared\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrender_with_direction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrender_with_direction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;66;03m# Create an importance sampler for (optional) fine rendering\u001b[39;00m\n\u001b[1;32m     89\u001b[0m     samplers\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m     90\u001b[0m         ImportanceRaySampler(\n\u001b[1;32m     91\u001b[0m             results_i\u001b[38;5;241m.\u001b[39mvolume_range, results_i\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mimportance_sampling_options\n\u001b[1;32m     92\u001b[0m         )\n\u001b[1;32m     93\u001b[0m     )\n",
      "File \u001b[0;32m~/repos/spic-e/shap_e/models/nerf/ray.py:238\u001b[0m, in \u001b[0;36mRayVolumeIntegral.render_rays\u001b[0;34m(self, origin, direction, t0, prev_raw, shared, render_with_direction)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;66;03m# 1. Intersect the rays with the current volume and sample ts to\u001b[39;00m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;66;03m# integrate along.\u001b[39;00m\n\u001b[1;32m    237\u001b[0m vrange \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvolume\u001b[38;5;241m.\u001b[39mintersect(origin, direction, t0_lower\u001b[38;5;241m=\u001b[39mt0)\n\u001b[0;32m--> 238\u001b[0m ts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msampler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvrange\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvrange\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_samples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prev_raw \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m shared:\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;66;03m# Append the previous ts now before fprop because previous\u001b[39;00m\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;66;03m# rendering used a different model and we can't reuse the output.\u001b[39;00m\n\u001b[1;32m    243\u001b[0m     ts \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msort(torch\u001b[38;5;241m.\u001b[39mcat([ts, prev_raw\u001b[38;5;241m.\u001b[39mts], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mvalues\n",
      "File \u001b[0;32m~/miniconda3/envs/spic-e/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/spic-e/shap_e/models/nerf/ray.py:505\u001b[0m, in \u001b[0;36mImportanceRaySampler.sample\u001b[0;34m(self, t0, t1, n_samples)\u001b[0m\n\u001b[1;32m    503\u001b[0m inds \u001b[38;5;241m=\u001b[39m sample_pmf(pmf, n_samples)\n\u001b[1;32m    504\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m inds\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m (batch_size, \u001b[38;5;241m*\u001b[39mshape, n_samples, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 505\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (inds \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mall() \u001b[38;5;129;01mand\u001b[39;00m \u001b[43m(\u001b[49m\u001b[43minds\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mn_coarse_samples\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    507\u001b[0m t_rand \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(inds\u001b[38;5;241m.\u001b[39mshape, device\u001b[38;5;241m=\u001b[39minds\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    508\u001b[0m lower_ \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mgather(lower, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, inds)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "xm = load_model('transmitter', device=device)\n",
    "model = load_model('text300M', device=device)\n",
    "model.wrapped.cond_drop_prob = cond_drop_prob\n",
    "model.freeze_all_parameters()\n",
    "network = LoRANetwork(model.wrapped, rank, alpha).to(device)\n",
    "network.load_state_dict(torch.load(lora_weight))\n",
    "diffusion = diffusion_from_config(load_config('diffusion'))\n",
    "test_model_kwargs = dict(texts=[prompt])\n",
    "cameras = create_pan_cameras(size, device)\n",
    "\n",
    "for i in tqdm(range(n), total=n):\n",
    "    seed = random.randint(0, 5000)\n",
    "    x_T = torch.randn((1, model.d_latent), device=device).expand(1, -1) * sigma_max\n",
    "    for scale in scales:\n",
    "        network.set_lora_slider(scale)\n",
    "        with network:\n",
    "            with torch.no_grad():\n",
    "                test_latents = sample_latents(\n",
    "                    device=device,\n",
    "                    batch_size=1,\n",
    "                    model=model,\n",
    "                    diffusion=diffusion,\n",
    "                    guidance_scale=guidance_scale,\n",
    "                    model_kwargs=test_model_kwargs,\n",
    "                    clip_denoised=True,\n",
    "                    use_fp16=True,\n",
    "                    use_karras=True,\n",
    "                    karras_steps=64,\n",
    "                    sigma_min=1e-3,\n",
    "                    sigma_max=sigma_max,\n",
    "                    s_churn=0,\n",
    "                    progress=True,\n",
    "                    x_T=x_T,\n",
    "                )\n",
    "        images = decode_latent_images(xm, test_latents[0], cameras, rendering_mode=render_mode)\n",
    "        result_path = os.path.join(output_dir, f'{i}_{scale}.gif')\n",
    "        export_to_gif(images, result_path)\n",
    "        flush(test_latents)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spic-e",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
