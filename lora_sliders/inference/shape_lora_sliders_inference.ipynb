{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5\"\n",
    "os.environ[\"WANDB_API_KEY\"] = \"7b14a62f11dc360ce036cf59b53df0c12cd87f5a\"\n",
    "import torch\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from lora import LoRANetwork\n",
    "from diffusers.utils import export_to_gif\n",
    "from shap_e.diffusion.sample import sample_latents\n",
    "from shap_e.models.download import load_model, load_config\n",
    "from shap_e.diffusion.gaussian_diffusion import diffusion_from_config\n",
    "from shap_e.util.notebooks import create_pan_cameras, decode_latent_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5\n",
    "rank = 4\n",
    "size = 160\n",
    "prompt = \"\"\n",
    "alpha = 1.0\n",
    "sigma_max = 160\n",
    "scales = [-1, 1]\n",
    "render_mode = \"nerf\"\n",
    "cond_drop_prob = 0.5\n",
    "guidance_scale = 7.5\n",
    "name = \"colorslider_visual_2024-03-26 08:39:53.777522\"\n",
    "lora_weight = f\"/home/noamatia/repos/shape_lora_sliders/outputs/{name}/models/model_best.pt\"\n",
    "\n",
    "output_dir = os.path.join('outputs', name, 'test')\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "def flush(*args):\n",
    "    for arg in args:\n",
    "        del arg\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create LoRA for SplitVectorDiffusion: 245 modules.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97b2a412a5ef4eecb823c33403048207",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0202fc05a744a4a9a69078ad9f44fe7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [02:22<09:31, 142.98s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa5655358be14d87b867fb84ba935c99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "538bd36eb7564cf8951368f00aed71a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [04:09<16:39, 249.75s/it]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 768.00 MiB. GPU 0 has a total capacity of 23.68 GiB of which 685.38 MiB is free. Process 455365 has 11.35 GiB memory in use. Including non-PyTorch memory, this process has 5.60 GiB memory in use. Process 943126 has 6.05 GiB memory in use. Of the allocated memory 5.29 GiB is allocated by PyTorch, and 62.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 36\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     19\u001b[0m         test_latents \u001b[38;5;241m=\u001b[39m sample_latents(\n\u001b[1;32m     20\u001b[0m             device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[1;32m     21\u001b[0m             batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     34\u001b[0m             x_T\u001b[38;5;241m=\u001b[39mx_T,\n\u001b[1;32m     35\u001b[0m         )\n\u001b[0;32m---> 36\u001b[0m images \u001b[38;5;241m=\u001b[39m \u001b[43mdecode_latent_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_latents\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcameras\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrendering_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrender_mode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m result_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscale\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.gif\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     38\u001b[0m export_to_gif(images, result_path)\n",
      "File \u001b[0;32m~/miniconda3/envs/shape_lora_sliders/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/shape_lora_sliders/shap_e/util/notebooks.py:53\u001b[0m, in \u001b[0;36mdecode_latent_images\u001b[0;34m(xm, latent, cameras, rendering_mode)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mno_grad()\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode_latent_images\u001b[39m(\n\u001b[1;32m     48\u001b[0m     xm: Union[Transmitter, VectorDecoder],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     51\u001b[0m     rendering_mode: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstf\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     52\u001b[0m ):\n\u001b[0;32m---> 53\u001b[0m     decoded \u001b[38;5;241m=\u001b[39m \u001b[43mxm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrenderer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender_views\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43mAttrDict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcameras\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcameras\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mxm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mxm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTransmitter\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mxm\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbottleneck_to_params\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlatent\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mAttrDict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrendering_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrendering_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrender_with_direction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m     arr \u001b[38;5;241m=\u001b[39m decoded\u001b[38;5;241m.\u001b[39mchannels\u001b[38;5;241m.\u001b[39mclamp(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m255\u001b[39m)\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39muint8)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [Image\u001b[38;5;241m.\u001b[39mfromarray(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arr]\n",
      "File \u001b[0;32m~/repos/shape_lora_sliders/shap_e/models/nerstf/renderer.py:215\u001b[0m, in \u001b[0;36mNeRSTFRenderer.render_views\u001b[0;34m(self, batch, params, options)\u001b[0m\n\u001b[1;32m    211\u001b[0m rendering_mode \u001b[38;5;241m=\u001b[39m options\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrendering_mode\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rendering_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnerf\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mrender_views_from_rays\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender_rays\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m rendering_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstf\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    225\u001b[0m     sdf_fn \u001b[38;5;241m=\u001b[39m tf_fn \u001b[38;5;241m=\u001b[39m nerstf_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/repos/shape_lora_sliders/shap_e/models/renderer.py:217\u001b[0m, in \u001b[0;36mrender_views_from_rays\u001b[0;34m(render_rays, batch, params, options, device)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_batches):\n\u001b[1;32m    213\u001b[0m     rays_batch \u001b[38;5;241m=\u001b[39m AttrDict(\n\u001b[1;32m    214\u001b[0m         rays\u001b[38;5;241m=\u001b[39mrays[:, idx \u001b[38;5;241m*\u001b[39m ray_batch_size : (idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m ray_batch_size],\n\u001b[1;32m    215\u001b[0m         radii\u001b[38;5;241m=\u001b[39mradii[:, idx \u001b[38;5;241m*\u001b[39m ray_batch_size : (idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m ray_batch_size],\n\u001b[1;32m    216\u001b[0m     )\n\u001b[0;32m--> 217\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mrender_rays\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrays_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(camera, DifferentiableProjectiveCamera):\n\u001b[1;32m    220\u001b[0m         z_batch \u001b[38;5;241m=\u001b[39m z_directions[:, idx \u001b[38;5;241m*\u001b[39m ray_batch_size : (idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m ray_batch_size]\n",
      "File \u001b[0;32m~/repos/shape_lora_sliders/shap_e/models/nerstf/renderer.py:141\u001b[0m, in \u001b[0;36mNeRSTFRenderer.render_rays\u001b[0;34m(self, batch, params, options)\u001b[0m\n\u001b[1;32m    132\u001b[0m options\u001b[38;5;241m.\u001b[39mnerf_level \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfine\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    133\u001b[0m parts \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    134\u001b[0m     RayVolumeIntegral(\n\u001b[1;32m    135\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    139\u001b[0m     ),\n\u001b[1;32m    140\u001b[0m ]\n\u001b[0;32m--> 141\u001b[0m fine_results, _, raw_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mrender_rays\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrays\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvoid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshared\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseparate_shared_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprev_raw_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoarse_raw_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrender_with_direction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender_with_direction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m raw \u001b[38;5;241m=\u001b[39m raw_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    151\u001b[0m aux_losses \u001b[38;5;241m=\u001b[39m fine_results\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39maux_losses\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[0;32m~/repos/shape_lora_sliders/shap_e/models/nerf/ray.py:79\u001b[0m, in \u001b[0;36mrender_rays\u001b[0;34m(rays, parts, void_model, shared, prev_raw_outputs, render_with_direction, importance_sampling_options)\u001b[0m\n\u001b[1;32m     74\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m part_i, prev_raw_i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(parts, prev_raw_outputs):\n\u001b[1;32m     77\u001b[0m \n\u001b[1;32m     78\u001b[0m     \u001b[38;5;66;03m# Integrate over [t[i], t[i + 1]]\u001b[39;00m\n\u001b[0;32m---> 79\u001b[0m     results_i \u001b[38;5;241m=\u001b[39m \u001b[43mpart_i\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender_rays\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m        \u001b[49m\u001b[43morigin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdirec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mt0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprev_raw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprev_raw_i\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshared\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshared\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrender_with_direction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrender_with_direction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;66;03m# Create an importance sampler for (optional) fine rendering\u001b[39;00m\n\u001b[1;32m     89\u001b[0m     samplers\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m     90\u001b[0m         ImportanceRaySampler(\n\u001b[1;32m     91\u001b[0m             results_i\u001b[38;5;241m.\u001b[39mvolume_range, results_i\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mimportance_sampling_options\n\u001b[1;32m     92\u001b[0m         )\n\u001b[1;32m     93\u001b[0m     )\n",
      "File \u001b[0;32m~/repos/shape_lora_sliders/shap_e/models/nerf/ray.py:255\u001b[0m, in \u001b[0;36mRayVolumeIntegral.render_rays\u001b[0;34m(self, origin, direction, t0, prev_raw, shared, render_with_direction)\u001b[0m\n\u001b[1;32m    253\u001b[0m optional_directions \u001b[38;5;241m=\u001b[39m directions \u001b[38;5;28;01mif\u001b[39;00m render_with_direction \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    254\u001b[0m mids \u001b[38;5;241m=\u001b[39m (ts[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m1\u001b[39m:, :] \u001b[38;5;241m+\u001b[39m ts[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m--> 255\u001b[0m raw \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43mQuery\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpositions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdirection\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptional_directions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_min\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mvrange\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt0\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmids\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_max\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvrange\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt1\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m raw\u001b[38;5;241m.\u001b[39mts \u001b[38;5;241m=\u001b[39m ts\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prev_raw \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m shared:\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;66;03m# We can append the additional queries to previous raw outputs\u001b[39;00m\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;66;03m# before integration\u001b[39;00m\n",
      "File \u001b[0;32m~/repos/shape_lora_sliders/shap_e/models/nerstf/renderer.py:76\u001b[0m, in \u001b[0;36mNeRSTFRenderer._query\u001b[0;34m(self, query, params, options)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m query\u001b[38;5;241m.\u001b[39mdirection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnerstf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 76\u001b[0m     sdf \u001b[38;5;241m=\u001b[39m tf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnerstf\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubdict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnerstf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     82\u001b[0m     sdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msdf(no_dir_query, params\u001b[38;5;241m=\u001b[39msubdict(params, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msdf\u001b[39m\u001b[38;5;124m\"\u001b[39m), options\u001b[38;5;241m=\u001b[39moptions)\n",
      "File \u001b[0;32m~/miniconda3/envs/shape_lora_sliders/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/shape_lora_sliders/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/repos/shape_lora_sliders/shap_e/models/nerstf/mlp.py:89\u001b[0m, in \u001b[0;36mMLPNeRSTFModel.forward\u001b[0;34m(self, query, params, options)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     84\u001b[0m     query: Query,\n\u001b[1;32m     85\u001b[0m     params: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     86\u001b[0m     options: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, Any]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     87\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AttrDict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[1;32m     88\u001b[0m     options \u001b[38;5;241m=\u001b[39m AttrDict() \u001b[38;5;28;01mif\u001b[39;00m options \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m AttrDict(options)\n\u001b[0;32m---> 89\u001b[0m     h, h_directionless \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mlp\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mposition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdirection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m     activations \u001b[38;5;241m=\u001b[39m map_indices_to_keys(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mh_map, h)\n\u001b[1;32m     93\u001b[0m     activations\u001b[38;5;241m.\u001b[39mupdate(map_indices_to_keys(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mh_directionless_map, h_directionless))\n",
      "File \u001b[0;32m~/repos/shape_lora_sliders/shap_e/models/stf/mlp.py:176\u001b[0m, in \u001b[0;36mMLPModel._mlp\u001b[0;34m(self, position, direction, params, options)\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    174\u001b[0m         parameters\u001b[38;5;241m.\u001b[39mextend(layer\u001b[38;5;241m.\u001b[39mparameters())\n\u001b[0;32m--> 176\u001b[0m h_final, h_directionless \u001b[38;5;241m=\u001b[39m \u001b[43mcheckpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmlp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mposition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheckpoint_stf_model\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m h_final, h_directionless\n",
      "File \u001b[0;32m~/repos/shape_lora_sliders/shap_e/models/nn/checkpoint.py:26\u001b[0m, in \u001b[0;36mcheckpoint\u001b[0;34m(func, inputs, params, flag)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m CheckpointFunction\u001b[38;5;241m.\u001b[39mapply(func, \u001b[38;5;28mlen\u001b[39m(inputs), \u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/shape_lora_sliders/shap_e/models/stf/mlp.py:146\u001b[0m, in \u001b[0;36mMLPModel._run_mlp\u001b[0;34m(self, position, direction, params)\u001b[0m\n\u001b[1;32m    144\u001b[0m     h_preact \u001b[38;5;241m=\u001b[39m h\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 146\u001b[0m         h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactivation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m h_final \u001b[38;5;241m=\u001b[39m h\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m h_directionless \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/repos/shape_lora_sliders/shap_e/models/nn/ops.py:20\u001b[0m, in \u001b[0;36mswish\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mswish\u001b[39m(x):\n\u001b[0;32m---> 20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 768.00 MiB. GPU 0 has a total capacity of 23.68 GiB of which 685.38 MiB is free. Process 455365 has 11.35 GiB memory in use. Including non-PyTorch memory, this process has 5.60 GiB memory in use. Process 943126 has 6.05 GiB memory in use. Of the allocated memory 5.29 GiB is allocated by PyTorch, and 62.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "xm = load_model('transmitter', device=device)\n",
    "model = load_model('text300M', device=device)\n",
    "model.wrapped.cond_drop_prob = cond_drop_prob\n",
    "model.freeze_all_parameters()\n",
    "network = LoRANetwork(model.wrapped, rank, alpha).to(device)\n",
    "network.load_state_dict(torch.load(lora_weight))\n",
    "diffusion = diffusion_from_config(load_config('diffusion'))\n",
    "test_model_kwargs = dict(texts=[prompt])\n",
    "cameras = create_pan_cameras(size, device)\n",
    "\n",
    "for i in tqdm(range(n), total=n):\n",
    "    seed = random.randint(0, 5000)\n",
    "    x_T = torch.randn((1, model.d_latent), device=device).expand(1, -1) * sigma_max\n",
    "    for scale in scales:\n",
    "        network.set_lora_slider(scale)\n",
    "        with network:\n",
    "            with torch.no_grad():\n",
    "                test_latents = sample_latents(\n",
    "                    device=device,\n",
    "                    batch_size=1,\n",
    "                    model=model,\n",
    "                    diffusion=diffusion,\n",
    "                    guidance_scale=guidance_scale,\n",
    "                    model_kwargs=test_model_kwargs,\n",
    "                    clip_denoised=True,\n",
    "                    use_fp16=True,\n",
    "                    use_karras=True,\n",
    "                    karras_steps=64,\n",
    "                    sigma_min=1e-3,\n",
    "                    sigma_max=sigma_max,\n",
    "                    s_churn=0,\n",
    "                    progress=True,\n",
    "                    x_T=x_T,\n",
    "                )\n",
    "        images = decode_latent_images(xm, test_latents[0], cameras, rendering_mode=render_mode)\n",
    "        result_path = os.path.join(output_dir, f'{i}_{scale}.gif')\n",
    "        export_to_gif(images, result_path)\n",
    "        flush(test_latents)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spic-e",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
